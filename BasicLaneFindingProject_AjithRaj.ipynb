{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Importing all the Required modules__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing all the necessary modules\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "import cv2\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Helper Function 1 :: *grayscale(Input Image)* --> Grayscaling is required for preprocessing the image for further mathematical and logical operations. To avoid selecting manual thresholds, OTSU thresholding method is used, which in a nutshell will automaticallly select the thresholds using the input image and mathematical calculations.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting the Input image to graycale for further processing\n",
    "#Since selecting threshold is difficult for an image stream for uncontrolled environment images, OTSU threholding method\n",
    "#is used to automatically select thresholds for the image.\n",
    "def grayscale(img):\n",
    "    rows    = img.shape [1]\n",
    "    cols    = img.shape [0]\n",
    "    ch      = img.shape [2]   \n",
    "    \n",
    "    grayscale_main            = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    ret,grayscale_output      = cv2.threshold(grayscale_main, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    \n",
    "    return grayscale_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Helper Function 2 :: *canny(Input Image)* --> Canny function from openCV library automatically converts the input image into an edge detected image output for easier classification in further steps.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using openCV methods to directly to transform the input image\n",
    "def canny(img, low_threshold, high_threshold):\n",
    "    return cv2.Canny(img, low_threshold, high_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Helper Function 3 :: *gaussian_blur(Input Image, Kernel Size)* --> Gaussian Blur is used to voluntarily remove noise signals from the input image, this is better than averaging  filter, where the data of the significant elements can get lost__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gaussian blur method is a better way of averaging the pixels in the image for selecting feature in an image\n",
    "def gaussian_blur(img, kernel_size):\n",
    "    return cv2.GaussianBlur(img, (kernel_size, kernel_size), 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Helper Function 4 :: *region_of_interest(Input Image, Vertices)* --> Based on the image, the regions wherein the object of significance can be expected and taken for calculations instead of checking all the pixel of the input images.__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To select the ROI image where we expect the lanes or our object of interest to remain prominantly.\n",
    "def region_of_interest(img, vertices):\n",
    "    mask = np.zeros_like(img)     \n",
    "    \n",
    "    if len(img.shape) > 2:\n",
    "        channel_count     = img.shape[2]  \n",
    "        ignore_mask_color = (255,) * channel_count\n",
    "    else:\n",
    "        ignore_mask_color = 255    \n",
    "    cv2.fillPoly(mask, vertices, ignore_mask_color)\n",
    "    masked_image          = cv2.bitwise_and(img, mask)\n",
    "    \n",
    "    return masked_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Helper Function 5 :: draw_lines(Image, lines, color, thickness) --> Using the detected line coordinates from the Hough Transform, this function applies the basic line equation to the detected lines and draws lines of user selected color and thickness values.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Once lanes coordinates are estimated, this function draws lines using the universal slope equation.\n",
    "def draw_lines(img, lines, color=[255, 0, 0], thickness=2):\n",
    "    #Setting Value to Global Variable\n",
    "    Image_Center_Threshold = img.shape[1] / 2\n",
    "\n",
    "    RightLane_X_Points = []\n",
    "    RightLane_Y_Points = []\n",
    "    \n",
    "    LeftLane_X_Points  = []\n",
    "    LeftLane_Y_Points  = []\n",
    "    \n",
    "    RightLane_SlopeIntercepts = []\n",
    "    LeftLane_SlopeIntercepts  = []\n",
    "    \n",
    "    Averaged_RightLane_SlopeIntercepts = []\n",
    "    Averaged_LeftLane_SlopeIntercepts  = []\n",
    "    \n",
    "    for line in lines:        \n",
    "        for x1,y1,x2,y2 in line:        \n",
    "           #For Right Lanes           \n",
    "           if(x1  > Image_Center_Threshold): #Image_Center_Threshold and x2 > Image_Center_Threshold ):\n",
    "               RightLane_SlopeIntercepts.append(np.polyfit((x1,x2),(y1,y2),1))\n",
    "               RightLane_X_Points.append([x1,x2])\n",
    "               RightLane_Y_Points.append([y1,y2])\n",
    "           #For Left lanes\n",
    "           else:\n",
    "               LeftLane_SlopeIntercepts.append(np.polyfit((x1,x2),(y1,y2),1))\n",
    "               LeftLane_X_Points.append([x1,x2])\n",
    "               LeftLane_Y_Points.append([y1,y2])\n",
    "               \n",
    "    Averaged_RightLane_SlopeIntercepts = np.mean(RightLane_SlopeIntercepts, axis = 0)\n",
    "    Averaged_LeftLane_SlopeIntercepts  = np.mean(LeftLane_SlopeIntercepts, axis = 0)\n",
    "    Left_Lane_points  = []\n",
    "    Right_Lane_points = [] \n",
    "        \n",
    "    # Y Coordinate Calculations\n",
    "    Right_Y_TopCoordinate    = (max(RightLane_Y_Points))\n",
    "    Right_Y_BottomCoordinate = (min(RightLane_Y_Points))\n",
    "    Left_Y_TopCoordinate     = (max(LeftLane_Y_Points))\n",
    "    Left_Y_BottomCoordinate  = (min(LeftLane_Y_Points))\n",
    "    #Fixing Common Y Coordinates for the Final Overlay Lines\n",
    "    Y_TopCoordinate     = max(Right_Y_TopCoordinate[0]      ,Left_Y_TopCoordinate[0])\n",
    "    Y_BottomCoordinate  = max(Right_Y_BottomCoordinate[0],Left_Y_BottomCoordinate[0])\n",
    "    \n",
    "    #Right Lane Bottom Point   \n",
    "    Right_X_TopCoordinate = int((Y_TopCoordinate - Averaged_RightLane_SlopeIntercepts[1]) / Averaged_RightLane_SlopeIntercepts[0])\n",
    "    Right_Lane_points.append(Right_X_TopCoordinate)\n",
    "    Right_Lane_points.append(Y_TopCoordinate)  \n",
    "    \n",
    "    #Right Lane Top Point\n",
    "    Right_X_BottomCoordinate = int((Y_BottomCoordinate - Averaged_RightLane_SlopeIntercepts[1]) / Averaged_RightLane_SlopeIntercepts[0])\n",
    "    Right_Lane_points.append(Right_X_BottomCoordinate)\n",
    "    Right_Lane_points.append(Y_BottomCoordinate)    \n",
    "    \n",
    "    #Left Lane Bottom Point\n",
    "    Left_X_TopCoordinate     = int((Y_TopCoordinate - Averaged_LeftLane_SlopeIntercepts[1]) / Averaged_LeftLane_SlopeIntercepts[0])\n",
    "    Left_Lane_points.append(Left_X_TopCoordinate)\n",
    "    Left_Lane_points.append(Y_TopCoordinate)\n",
    "\n",
    "    #Left Lane Top Points\n",
    "    Left_X_BottomCoordinate  = int((Y_BottomCoordinate - Averaged_LeftLane_SlopeIntercepts[1]) / Averaged_LeftLane_SlopeIntercepts[0])\n",
    "    Left_Lane_points.append(Left_X_BottomCoordinate)\n",
    "    Left_Lane_points.append(Y_BottomCoordinate)\n",
    "    \n",
    "    draw_output = cv2.line(img, (Right_Lane_points[0] , Right_Lane_points[1]) , (Right_Lane_points[2],Right_Lane_points[3]) , color , thickness)    \n",
    "    draw_output = cv2.line(img, (Left_Lane_points[0]  , Left_Lane_points[1])  , (Left_Lane_points[2] ,Left_Lane_points[3])  , color , thickness)\n",
    "\n",
    "    return draw_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Helper Function 6 :: hough_lines(img, rho, theta, threshold, min_line_len, max_line_gap) --> This function takes the pre-processed images with the lane marking highlighted/focused, and by using the various tuning thresholds selects the pixels which are a line nature whilst ignoring the scattered the point detections in the preprocessed image.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Used to represent images in terms of lines\n",
    "def hough_lines(img, rho, theta, threshold, min_line_len, max_line_gap):\n",
    "    lines       = cv2.HoughLinesP(img, rho, theta, threshold, np.array([]), minLineLength=min_line_len, maxLineGap=max_line_gap)\n",
    "    line_img    = np.zeros((img.shape[0], img.shape[1], 3), dtype=np.uint8)\n",
    "    hough_image = draw_lines(line_img, lines, color=[255, 0, 0], thickness=10)\n",
    "    \n",
    "    return hough_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Helper Function 7 :: weighted_img(img, initial_img, α, β, γ) --> Superimposes the line which is detected in thorugh the pipe onto to the image stream, the superimposition parameters can be tuned usingultiplication factors α, β and addition constant γ.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Used to superimpose the lane marking that pipeline detects to the original image, or basically the annotated image output \n",
    "def weighted_img(img, initial_img, α=0.8, β=1., γ=0.):    \n",
    "    return cv2.addWeighted(initial_img, α, img, β, γ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Test Function to the Check the pipeline is working.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test Function to test whether the pipeline works\n",
    "def main():\n",
    "    main_image   = cv2.imread('exit-ramp.jpg')\n",
    "\n",
    "    process_image(main_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__The Final Pipeline function which is used to run the detection algorithm__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basically the entire pipeline using all the helper functions\n",
    "def process_image(main_image):\n",
    "    # NOTE: The output you return should be a color image (3 channel) for processing video below\n",
    "    # TODO: put your pipeline here,\n",
    "    # you should return the final output (image where lines are drawn on lanes)\n",
    "\n",
    "    \n",
    "    gray_image         = grayscale(main_image)\n",
    "    canny_image        = canny(gray_image, 140, 170)   \n",
    "    gaussianblur_image = gaussian_blur(canny_image,5)\n",
    "    \n",
    "    rows = gaussianblur_image.shape[1]\n",
    "    cols = gaussianblur_image.shape[0]\n",
    "    \n",
    "    top_left_corner   = [rows*(4.5/10) , cols*(3/5)]\n",
    "    top_right_corner  = [rows*(6/10)   , cols*(3/5)]\n",
    "    down_left_corner  = [0   , cols]\n",
    "    down_right_corner = [rows , cols]\n",
    "    roi_points = np.array( [[top_left_corner,top_right_corner,down_right_corner,down_left_corner]], dtype=np.int32 )\n",
    "    \n",
    "    roi_image = region_of_interest(gaussianblur_image,roi_points)\n",
    "\n",
    "    # Define the Hough transform parameters\n",
    "    # Make a blank the same size as our image to draw on\n",
    "    rho = 1 # distance resolution in pixels of the Hough grid\n",
    "    theta = np.pi/180 # angular resolution in radians of the Hough grid\n",
    "    threshold = 15     # minimum number of votes (intersections in Hough grid cell)\n",
    "    min_line_length = 45  #minimum number of pixels making up a line\n",
    "    max_line_gap = 10    # maximum gap in pixels between connectable line segments\n",
    "    line_image = np.copy(main_image)*0 # creating a blank to draw lines on\n",
    "    \n",
    "    # Run Hough on edge detected image\n",
    "    # Output \"lines\" is an array containing endpoints of detected line segments\n",
    "    hough_image = hough_lines(roi_image, rho, theta, threshold,min_line_length, max_line_gap)\n",
    "    \n",
    "    final_image = weighted_img(hough_image,main_image,.5,1,0)\n",
    "    \n",
    "    return final_image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Import the Video Processing modules__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import everything needed to edit/save/watch video clips\n",
    "from moviepy.editor import VideoFileClip\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__These lines take the video in the directory and run the software pipeline on a small section of it which can be manually set in the subclip(X,X).__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] >>>> Building video test_videos_output/solidWhiteRight.mp4\n",
      "[MoviePy] Writing video test_videos_output/solidWhiteRight.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 221/222 [00:17<00:00, 11.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] Done.\n",
      "[MoviePy] >>>> Video ready: test_videos_output/solidWhiteRight.mp4 \n",
      "\n",
      "CPU times: user 7.64 s, sys: 245 ms, total: 7.88 s\n",
      "Wall time: 19.4 s\n"
     ]
    }
   ],
   "source": [
    "    white_output = 'test_videos_output/solidWhiteRight.mp4'\n",
    "    ## To speed up the testing process you may want to try your pipeline on a shorter subclip of the video\n",
    "    ## To do so add .subclip(start_second,end_second) to the end of the line below\n",
    "    ## Where start_second and end_second are integer values representing the start and end of the subclip\n",
    "    ## You may also uncomment the following line for a subclip of the first 5 seconds\n",
    "    clip1 = VideoFileClip(\"test_videos/solidWhiteRight.mp4\").subclip(5,10)\n",
    "    clip1 = VideoFileClip(\"test_videos/solidWhiteRight.mp4\")\n",
    "    white_clip = clip1.fl_image(process_image) #NOTE: this function expects color images!!\n",
    "    %time white_clip.write_videofile(white_output, audio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
